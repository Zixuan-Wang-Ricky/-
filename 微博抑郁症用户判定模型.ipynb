{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 从微博爬取相关数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#录入正常用户的ID，IDZC和抑郁症用户的ID,IDYY\n",
    "IDZC = [5686659045, 5070488897, 2236649864, 5481842586, 1799285021,1641367807, 6079669733, 1810427660, 1786644137, 1922022240, 2306947451, 2891878750, 1889607690,3199441861,2396116183,6016630245,1639478840,2850644073,5239210616,2890692204,1943101785,1781022931,5948804520,2687922693,1730957060,2878473932,5484858683, 1800755655, 6142602122, 1740924090, 3340623142, 2437733093, 5577646140, 3962982466, 3025511543, 2009288583, 1787333644, 5634925497]\n",
    "IDYY = [6992648962,5104887647,2795669352, 6456834915, 5904448518,6136894270,6052939320,6146626040,5844111799, 6735998426,5328608161,6795029367]\n",
    "print(len(IDZC))\n",
    "print(len(IDYY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据准备\n",
    "\n",
    "import urllib.request\n",
    "import json\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "\n",
    "#设置代理IP\n",
    "proxy_addr=\"122.241.72.191:808\"\n",
    "\n",
    "#定义页面打开函数\n",
    "def use_proxy(url,proxy_addr):\n",
    "    req=urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\",\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0\")\n",
    "    proxy=urllib.request.ProxyHandler({'http':proxy_addr})\n",
    "    opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data=urllib.request.urlopen(req).read().decode('utf-8','ignore')\n",
    "    return data\n",
    "\n",
    "#获取微博主页的containerid，爬取微博内容时需要此id\n",
    "def get_containerid(url):\n",
    "    data=use_proxy(url,proxy_addr)\n",
    "    content=json.loads(data).get('data')\n",
    "    for data in content.get('tabsInfo').get('tabs'):\n",
    "        if(data.get('tab_type')=='weibo'):\n",
    "            containerid=data.get('containerid')\n",
    "    return containerid\n",
    "\n",
    "#获取微博内容信息,并保存到文本中，内容包括：每条微博的内容、微博详情页面地址、点赞数、评论数、转发数等\n",
    "def get_weibo(id,file):\n",
    "    i=1\n",
    "    while True:\n",
    "        url='https://m.weibo.cn/api/container/getIndex?type=uid&value='+str(id)\n",
    "        weibo_url='https://m.weibo.cn/api/container/getIndex?type=uid&value='+str(id)+'&containerid='+get_containerid(url)+'&page='+str(i)\n",
    "        try:\n",
    "            data=use_proxy(weibo_url,proxy_addr)\n",
    "            content=json.loads(data).get('data')\n",
    "            cards=content.get('cards')\n",
    "            if(len(cards)>0) and i<13 :\n",
    "                for j in range(len(cards)):\n",
    "                    print(\"-----正在爬取第\"+str(i)+\"页，第\"+str(j)+\"条微博------\")\n",
    "                    card_type=cards[j].get('card_type')\n",
    "                    if(card_type==9):\n",
    "                        mblog=cards[j].get('mblog')\n",
    "                        text=mblog.get('text')\n",
    "                        text=remove_tags(text)\n",
    "                        created_at=mblog.get('created_at')\n",
    "                        with open(file,'a',encoding='utf-8') as fh:\n",
    "                            fh.write(text+\"\\n\")\n",
    "                i+=1\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取抑郁症患者微博\n",
    "if __name__==\"__main__\":\n",
    "    IDYY = [6992648962,5104887647,2795669352, 6456834915, 5904448518,6136894270,6052939320,6146626040,5844111799, 6735998426,5328608161,6795029367]\n",
    "    for id in IDYY:\n",
    "        file=str(id)+\".txt\"\n",
    "        get_weibo(id,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取非抑郁症患者微博\n",
    "if __name__==\"__main__\":\n",
    "    IDZC = [5686659045, 5070488897, 2236649864, 5481842586, 1799285021,1641367807, 6079669733, 1810427660, 1786644137, 1922022240, 2306947451, 2891878750, 1889607690,3199441861,2396116183,6016630245,1639478840,2850644073,5239210616,2890692204,1943101785,1781022931,5948804520,2687922693,1730957060,2878473932,5484858683, 1800755655, 6142602122, 1740924090, 3340623142, 2437733093, 5577646140, 3962982466, 3025511543, 2009288583, 1787333644, 5634925497]\n",
    "    for id in IDZC:\n",
    "        file=str(id)+\".txt\"\n",
    "        get_weibo(id,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 微博情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一步\n",
    "#准备情感词典\n",
    "\n",
    "#哀\n",
    "AI = {}\n",
    "\n",
    "#恶\n",
    "E = {}\n",
    "\n",
    "#惊\n",
    "JING = {}\n",
    "\n",
    "#惧\n",
    "JU = {}\n",
    "\n",
    "#怒\n",
    "NU = {}\n",
    "\n",
    "#否定词\n",
    "noword = []\n",
    "\n",
    "#副词\n",
    "adverb = {}\n",
    "\n",
    "#无用词\n",
    "stopword = []\n",
    "\n",
    "#情感词\n",
    "file3 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\情感词典及对应分数\\哀.csv',\"r\", encoding ='utf-8')\n",
    "senList = file3.readlines()\n",
    "for s in senList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        AI[s.split(',')[0]] = s.split(',')[1]\n",
    "    except:\n",
    "        pass\n",
    "#恶\n",
    "file3 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\情感词典及对应分数\\恶.csv',\"r\", encoding ='utf-8')\n",
    "senList = file3.readlines()\n",
    "for s in senList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        E[s.split(',')[0]] = s.split(',')[1]\n",
    "    except:\n",
    "        pass\n",
    "#惊\n",
    "file3 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\情感词典及对应分数\\惊.csv',\"r\", encoding ='utf-8')\n",
    "senList = file3.readlines()\n",
    "for s in senList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        JING[s.split(',')[0]] = s.split(',')[1]\n",
    "    except:\n",
    "        pass\n",
    "#惧\n",
    "file3 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\情感词典及对应分数\\惧.csv',\"r\", encoding ='utf-8')\n",
    "senList = file3.readlines()\n",
    "for s in senList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        JU[s.split(',')[0]] = s.split(',')[1]\n",
    "    except:\n",
    "        pass\n",
    "#怒\n",
    "file3 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\情感词典及对应分数\\怒.csv',\"r\", encoding ='utf-8')\n",
    "senList = file3.readlines()\n",
    "for s in senList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        NU[s.split(',')[0]] = s.split(',')[1]\n",
    "    except:\n",
    "        pass\n",
    "#否定词\n",
    "file4 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\否定词词典\\deny_adv.txt',\"r\", encoding ='utf-8')\n",
    "notList = file4.readlines()\n",
    "for s in notList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        noword.append(s)\n",
    "    except:\n",
    "        print (\"数据错误：\"+s)\n",
    "#频率副词\n",
    "file5 = open(r'E:\\大学\\research\\crj\\心理\\proposal1\\NLP自然语言处理\\词典\\程度副词词典\\程度词语.txt',\"r\")\n",
    "degreeList = file5.readlines() \n",
    "for s in degreeList:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        adverb[s.split('\\t')[0]] = s.split('\\t')[1]\n",
    "    except:\n",
    "        print (\"数据错误：\"+s)\n",
    "#停用词\n",
    "file2 = open(r'E:/大学/research/crj/心理/proposal1/NLP自然语言处理/词典/停用词词典/stopwords-master/stopwords.txt',\"r\", encoding ='utf-8')\n",
    "stopwords = file2.readlines()\n",
    "for s in stopwords:\n",
    "    try:\n",
    "        s = s.replace('\\r\\n','').replace('\\n','')\n",
    "        stopword.append(s) \n",
    "    except:\n",
    "        print (\"数据错误：\"+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二步\n",
    "#清理文本+打分\n",
    "def givescore(sentence):\n",
    "    wordsList = jieba.cut(sentence, cut_all=False)\n",
    "    newWords = {}\n",
    "    i = 0\n",
    "    for w in wordsList:\n",
    "        if w not in stopword:\n",
    "            newWords[str(i)] =w\n",
    "            i = i+1\n",
    "\n",
    "    AIWord = {}\n",
    "    EWord = {}\n",
    "    JINGWord = {}\n",
    "    JUWord = {}\n",
    "    NUWord = {}\n",
    "    notWord = {}\n",
    "    degreeWord = {}\n",
    "    m = 0\n",
    "    #这里就相当于是senwords或者degreeword的话就给予相应的评分（例如这句话里出现了 微 这个字，对应的是2分），\n",
    "    #是notword的话就给予-1，啥都不是的话就是0.\n",
    "    for index in newWords.keys():\n",
    "        if newWords[index] in AI.keys() and newWords[index] not in noword and newWords[index] not in adverb.keys():\n",
    "            AIWord[index] = AI[newWords[index]]\n",
    "        elif newWords[index] in E.keys() and newWords[index] not in noword and newWords[index] not in adverb.keys():\n",
    "            EWord[index] = E[newWords[index]]\n",
    "        elif newWords[index] in JING.keys() and newWords[index] not in noword and newWords[index] not in adverb.keys():\n",
    "            JINGWord[index] = JING[newWords[index]]\n",
    "        elif newWords[index] in JU.keys() and newWords[index] not in noword and newWords[index] not in adverb.keys():\n",
    "            JUWord[index] = JU[newWords[index]]\n",
    "        elif newWords[index] in NU.keys() and newWords[index] not in noword and newWords[index] not in adverb.keys():\n",
    "            NUWord[index] = NU[newWords[index]]\n",
    "        elif newWords[index] in noword and newWords[index] not in adverb.keys():\n",
    "            notWord[index] = -1\n",
    "        elif newWords[index] in adverb.keys():\n",
    "            degreeWord[index] = adverb[newWords[index]]\n",
    "    W = 1\n",
    "    score = []\n",
    "    # 存所有情感词的位置的列表\n",
    "    AILoc = []\n",
    "    ELoc = []\n",
    "    JINGLoc = []\n",
    "    JULoc = []\n",
    "    NULoc = []\n",
    "    notLoc = []\n",
    "    degreeLoc = []\n",
    "\n",
    "\n",
    "    for i in AIWord.keys():\n",
    "        AILoc.append(int(i))\n",
    "    for i in EWord.keys():\n",
    "        ELoc.append(int(i))\n",
    "    for i in JINGWord.keys():\n",
    "        JINGLoc.append(int(i))\n",
    "    for i in JUWord.keys():\n",
    "        JULoc.append(int(i))\n",
    "    for i in NUWord.keys():\n",
    "        NULoc.append(int(i))\n",
    "    for i in notWord.keys():\n",
    "        notLoc.append(int(i))\n",
    "    for i in degreeWord.keys():\n",
    "        degreeLoc.append(int(i))\n",
    "\n",
    "    AILoc.sort()\n",
    "    ELoc.sort()\n",
    "    JINGLoc.sort()\n",
    "    JULoc.sort()\n",
    "    NULoc.sort()\n",
    "    notLoc.sort()\n",
    "    degreeLoc.sort()\n",
    "\n",
    "    AIloc = -1\n",
    "    Eloc = -1\n",
    "    JINGloc = -1\n",
    "    JUloc = -1\n",
    "    NUloc = -1\n",
    "    \n",
    "    AIscore = 0\n",
    "    Escore = 0\n",
    "    JINGscore = 0\n",
    "    JUscore = 0\n",
    "    NUscore = 0\n",
    "\n",
    "    for i in range(0, len(newWords)):\n",
    "        if i in AILoc:\n",
    "            AIloc += 1\n",
    "            AIscore += W * float(AIWord[str(i)])\n",
    "            if AIloc < len(AILoc) - 1:\n",
    "                if AILoc[AIloc] - AILoc[AIloc + 1] > 1:\n",
    "                    for j in range(AILoc[AIloc]+1, AILoc[AIloc + 1]):\n",
    "                        if j in notLoc:\n",
    "                            W *= -1\n",
    "                        elif j in degreeLoc:\n",
    "                            W *= float(degreeWord[j])\n",
    "                else:\n",
    "                    W = 1\n",
    "        if AIloc < len(AILoc) - 1:\n",
    "            i = AILoc[AIloc + 1]\n",
    "\n",
    "    for i in range(0, len(newWords)):\n",
    "        if i in ELoc:\n",
    "            Eloc += 1\n",
    "            Escore += W * float(EWord[str(i)])\n",
    "            if Eloc < len(ELoc) - 1:\n",
    "                if ELoc[Eloc] - ELoc[Eloc + 1] > 1:\n",
    "                    for j in range(ELoc[Eloc]+1, ELoc[Eloc + 1]):\n",
    "                        if j in notLoc:\n",
    "                            W *= -1\n",
    "                        elif j in degreeLoc:\n",
    "                            W *= float(degreeWord[j])\n",
    "                else:\n",
    "                    W = 1\n",
    "        if Eloc < len(ELoc) - 1:\n",
    "            i = ELoc[Eloc + 1]\n",
    "\n",
    "    for i in range(0, len(newWords)):\n",
    "        if i in JINGLoc:\n",
    "            JINGloc += 1\n",
    "            JINGscore += W * float(JINGWord[str(i)])\n",
    "            if JINGloc < len(JINGLoc) - 1:\n",
    "                if JINGLoc[JINGloc] - JINGLoc[JINGloc + 1] > 1:\n",
    "                    for j in range(JINGLoc[JINGloc]+1, JINGLoc[JINGloc + 1]):\n",
    "                        if j in notLoc:\n",
    "                            W *= -1\n",
    "                        elif j in degreeLoc:\n",
    "                            W *= float(degreeWord[j])\n",
    "                else:\n",
    "                    W = 1\n",
    "        if JINGloc < len(JINGLoc) - 1:\n",
    "            i = JINGLoc[JINGloc + 1]\n",
    "\n",
    "    for i in range(0, len(newWords)):\n",
    "        if i in JULoc:\n",
    "            JUloc += 1\n",
    "            JUscore += W * float(JUWord[str(i)])\n",
    "            if JUloc < len(JULoc) - 1:\n",
    "                if JULoc[JUloc] - JULoc[JUloc + 1] > 1:\n",
    "                    for j in range(JULoc[JUloc]+1, JULoc[JUloc + 1]):\n",
    "                        if j in notLoc:\n",
    "                            W *= -1\n",
    "                        elif j in degreeLoc:\n",
    "                            W *= float(degreeWord[j])\n",
    "                else:\n",
    "                    W = 1\n",
    "        if JUloc < len(JULoc) - 1:\n",
    "            i = JULoc[JUloc + 1]\n",
    "\n",
    "    for i in range(0, len(newWords)):\n",
    "        if i in NULoc:\n",
    "            NUloc += 1\n",
    "            NUscore += W * float(NUWord[str(i)])\n",
    "            if NUloc < len(NULoc) - 1:\n",
    "                if NULoc[NUloc] - NULoc[NUloc + 1] > 1:\n",
    "                    for j in range(NULoc[NUloc]+1, NULoc[NUloc + 1]):\n",
    "                        if j in notLoc:\n",
    "                            W *= -1\n",
    "                        elif j in degreeLoc:\n",
    "                            W *= float(degreeWord[j])\n",
    "                else:\n",
    "                    W = 1\n",
    "        if NUloc < len(NULoc) - 1:\n",
    "            i = NULoc[NUloc + 1]   \n",
    "            \n",
    "    score.append(AIscore)#/len(AILoc))\n",
    "    score.append(Escore)#/len(ELoc))\n",
    "    score.append(JINGscore)#/len(JINGLoc))\n",
    "    score.append(JUscore)#/len(JULoc))\n",
    "    score.append(NUscore)#/len(NULoc))\n",
    "    \n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三步\n",
    "#抑郁症打分+输出csv文件\n",
    "import pandas as pd\n",
    "IDYY = [6992648962,5104887647,2795669352, 6456834915, 5904448518,6136894270,6052939320,6146626040,5844111799, 6735998426,5328608161,6795029367]\n",
    "\n",
    "scoredict = {}\n",
    "\n",
    "for i in IDYY:\n",
    "    n = 1\n",
    "    file = open(r'E:/大学/research/crj/心理/proposal1/raw data/抑郁12人/'+ str(i) +'.txt' ,\"r\", encoding ='utf-8')\n",
    "    userdata = file.readlines()\n",
    "    for data in userdata:\n",
    "        scoredict[str(i)+': '+ str(n)] = givescore(data)\n",
    "        n += 1\n",
    "scoredf = pd.DataFrame(pd.Series(scoredict),columns=['score'])\n",
    "scoredf = scoredf.reset_index().rename(columns = {'index':'index'})\n",
    "scoredf[\"depression\"] = 1\n",
    "scoredf.to_csv(\"抑郁打分\"+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四步\n",
    "#非抑郁症打分+输出csv文件\n",
    "import pandas as pd\n",
    "IDZC = [5686659045, 5070488897, 2236649864, 5481842586, 1799285021,1641367807, 6079669733, 1810427660, 1786644137, 1922022240, 2306947451, 2891878750, 1889607690,3199441861,2396116183,6016630245,1639478840,2850644073,5239210616,2890692204,1943101785,1781022931,5948804520,2687922693,1730957060,2878473932,5484858683, 1800755655, 6142602122, 1740924090, 3340623142, 2437733093, 5577646140, 3962982466, 3025511543, 2009288583, 1787333644, 5634925497]\n",
    "scoredict = {}\n",
    "\n",
    "for i in IDZC:\n",
    "    n = 1\n",
    "    file = open(r'E:/大学/research/crj/心理/proposal1/raw data/非抑郁38人/'+ str(i) +'.txt' ,\"r\", encoding ='utf-8')\n",
    "    userdata = file.readlines()\n",
    "    for data in userdata:\n",
    "        scoredict[str(i)+': '+ str(n)] = givescore(data)\n",
    "        n += 1\n",
    "scoredf = pd.DataFrame(pd.Series(scoredict),columns=['score'])\n",
    "scoredf = scoredf.reset_index().rename(columns = {'index':'index'})\n",
    "scoredf[\"depression\"] = 0\n",
    "scoredf.to_csv(\"非抑郁打分\"+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 多元逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "逻辑回归\n",
    "因变量：\n",
    "depression: 二分变量、 1表示此微博由抑郁症患者发出、0表示此微博由非抑郁症患者发出\n",
    "\n",
    "自变量：\n",
    "微博内容情感分析：\n",
    "SAD：连续变量、表示此微博在悲伤这一情感上获得的分数\n",
    "DIS：连续变量、表示此微博在厌恶这一情感上获得的分数\n",
    "FER: 连续变量、表示此微博在恐惧这一情感上获得的分数\n",
    "ANG: 连续变量、表示此微博在愤怒这一情感上获得的分数\n",
    "\n",
    "用户自我表露水平：\n",
    "PIC: 连续变量、表示用户在给定时间段内发布图片的数量\n",
    "FLW: 连续变量、表示用户的关注人数\n",
    "DCP: 连续变量、表示用户的个人简介的字数\n",
    "\n",
    "用户特征：\n",
    "GEN: 二分变量、0表示发布此微博的用户为女性、1表示发布此微博的用户为男性\n",
    "LOC: 分类变量、地址信息，1表示未获取准确的地址、2表示北京、3表示广州等\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "os.chdir(r\"E:/大学/research/crj/心理/proposal1/abstract/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num</th>\n",
       "      <th>depression</th>\n",
       "      <th>SAD</th>\n",
       "      <th>DIS</th>\n",
       "      <th>FER</th>\n",
       "      <th>ANG</th>\n",
       "      <th>PIC</th>\n",
       "      <th>GEN</th>\n",
       "      <th>LOC</th>\n",
       "      <th>wbcount</th>\n",
       "      <th>FLW</th>\n",
       "      <th>fans</th>\n",
       "      <th>description</th>\n",
       "      <th>DCP</th>\n",
       "      <th>des</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5686659045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2046</td>\n",
       "      <td>252</td>\n",
       "      <td>15870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5686659045</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2046</td>\n",
       "      <td>252</td>\n",
       "      <td>15870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5686659045</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2046</td>\n",
       "      <td>252</td>\n",
       "      <td>15870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5686659045</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2046</td>\n",
       "      <td>252</td>\n",
       "      <td>15870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5686659045</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2046</td>\n",
       "      <td>252</td>\n",
       "      <td>15870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  num  depression  SAD  DIS  FER  ANG  PIC  GEN  LOC  wbcount  \\\n",
       "0  5686659045    1           0    0    0    0    0   79    0    1     2046   \n",
       "1  5686659045    2           0    0    0    0    0   79    0    1     2046   \n",
       "2  5686659045    3           0    0    0    0    0   79    0    1     2046   \n",
       "3  5686659045    4           0    0    9    0    0   79    0    1     2046   \n",
       "4  5686659045    5           0    0    0    0    0   79    0    1     2046   \n",
       "\n",
       "   FLW   fans description  DCP  des  \n",
       "0  252  15870           0    0    0  \n",
       "1  252  15870           0    0    0  \n",
       "2  252  15870           0    0    0  \n",
       "3  252  15870           0    0    0  \n",
       "4  252  15870           0    0    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入数据\n",
    "data = pd.read_csv(r'cleaned data.csv', skipinitialspace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 训练集样本量: 3748 \n",
      " 测试集样本量: 1606\n"
     ]
    }
   ],
   "source": [
    "# •随机抽样，建立训练集与测试集\n",
    "train = data.sample(frac=0.7, random_state=1234).copy()\n",
    "test = data[~ data.index.isin(train.index)].copy()\n",
    "print(' 训练集样本量: %i \\n 测试集样本量: %i' %(len(train), len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3使用向前逐步法从备选变量中选择变量，构建基于AIC的最优模型，绘制ROC曲线，同时检验模型的膨胀系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变量筛选\n",
    "# 向前法\n",
    "def forward_select(data, response):\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = float('inf'), float('inf')\n",
    "    while remaining:\n",
    "        aic_with_candidates=[]\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {}\".format(\n",
    "                response,' + '.join(selected + [candidate]))\n",
    "            aic = smf.glm(\n",
    "                formula=formula, data=data, \n",
    "                family=sm.families.Binomial(sm.families.links.logit)\n",
    "            ).fit().aic\n",
    "            aic_with_candidates.append((aic, candidate))\n",
    "        aic_with_candidates.sort(reverse=True)\n",
    "        best_new_score, best_candidate=aic_with_candidates.pop()\n",
    "        if current_score > best_new_score: \n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "            print ('aic is {},continuing!'.format(current_score))\n",
    "        else:        \n",
    "            print ('forward selection over!')\n",
    "            break\n",
    "            \n",
    "    formula = \"{} ~ {} \".format(response,' + '.join(selected))\n",
    "    print('final formula is {}'.format(formula))\n",
    "    model = smf.glm(\n",
    "        formula=formula, data=data, \n",
    "        family=sm.families.Binomial(sm.families.links.logit)\n",
    "    ).fit()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aic is 2553.0840825920613,continuing!\n",
      "aic is 1584.827525634258,continuing!\n",
      "aic is 1459.9036156959028,continuing!\n",
      "aic is 1390.919600592002,continuing!\n",
      "aic is 1292.7426850984725,continuing!\n",
      "aic is 1278.4438890971546,continuing!\n",
      "forward selection over!\n",
      "final formula is depression ~ LOC + FLW + GEN + PIC + DCP + DIS \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>depression</td>    <th>  No. Observations:  </th>  <td>  3748</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  3741</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -632.22</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Wed, 22 Apr 2020</td> <th>  Deviance:          </th> <td>  1264.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>15:24:42</td>     <th>  Pearson chi2:      </th> <td>1.16e+03</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>9</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    7.7744</td> <td>    0.436</td> <td>   17.827</td> <td> 0.000</td> <td>    6.920</td> <td>    8.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LOC</th>       <td>   -1.2685</td> <td>    0.072</td> <td>  -17.610</td> <td> 0.000</td> <td>   -1.410</td> <td>   -1.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FLW</th>       <td>   -0.0138</td> <td>    0.001</td> <td>  -16.652</td> <td> 0.000</td> <td>   -0.015</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GEN</th>       <td>   -0.3178</td> <td>    0.258</td> <td>   -1.231</td> <td> 0.218</td> <td>   -0.824</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PIC</th>       <td>   -0.0263</td> <td>    0.003</td> <td>   -9.902</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DCP</th>       <td>   -0.0921</td> <td>    0.011</td> <td>   -8.751</td> <td> 0.000</td> <td>   -0.113</td> <td>   -0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>       <td>    0.0784</td> <td>    0.020</td> <td>    3.895</td> <td> 0.000</td> <td>    0.039</td> <td>    0.118</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:             depression   No. Observations:                 3748\n",
       "Model:                            GLM   Df Residuals:                     3741\n",
       "Model Family:                Binomial   Df Model:                            6\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -632.22\n",
       "Date:                Wed, 22 Apr 2020   Deviance:                       1264.4\n",
       "Time:                        15:24:42   Pearson chi2:                 1.16e+03\n",
       "No. Iterations:                     9   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      7.7744      0.436     17.827      0.000       6.920       8.629\n",
       "LOC           -1.2685      0.072    -17.610      0.000      -1.410      -1.127\n",
       "FLW           -0.0138      0.001    -16.652      0.000      -0.015      -0.012\n",
       "GEN           -0.3178      0.258     -1.231      0.218      -0.824       0.188\n",
       "PIC           -0.0263      0.003     -9.902      0.000      -0.031      -0.021\n",
       "DCP           -0.0921      0.011     -8.751      0.000      -0.113      -0.072\n",
       "DIS            0.0784      0.020      3.895      0.000       0.039       0.118\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = [\"depression\", \"SAD\", 'DIS','FER','ANG','PIC','LOC','GEN','FLW','DCP']\n",
    "data_for_select = train[candidates]\n",
    "\n",
    "lg = forward_select(data=data_for_select, response = 'depression')\n",
    "lg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(df, col_i):\n",
    "    from statsmodels.formula.api import ols\n",
    "    \n",
    "    cols = list(df.columns)\n",
    "    cols.remove(col_i)\n",
    "    cols_noti = cols\n",
    "    formula = col_i + '~' + '+'.join(cols_noti)\n",
    "    r2 = ols(formula, df).fit().rsquared\n",
    "    return 1. / (1. - r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAD \t 1.00349097283763\n",
      "DIS \t 1.0300485708315161\n",
      "FER \t 1.004286614573935\n",
      "ANG \t 1.0024592888973172\n",
      "PIC \t 1.3565246635630852\n",
      "LOC \t 1.416226589480272\n",
      "GEN \t 1.1260604377262784\n",
      "FLW \t 1.3205907699059143\n",
      "DCP \t 1.2226850266097566\n"
     ]
    }
   ],
   "source": [
    "exog = train[candidates].drop(['depression'], axis=1)\n",
    "\n",
    "for i in exog.columns:\n",
    "    print(i, '\\t', vif(df=exog, col_i=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#无共线问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     0.838412\n",
       "7     0.719318\n",
       "10    0.719318\n",
       "15    0.719318\n",
       "16    0.719318\n",
       "Name: proba, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测\n",
    "train['proba'] = lg.predict(train)\n",
    "test['proba'] = lg.predict(test)\n",
    "\n",
    "test['proba'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定阈值\n",
    "\n",
    "test['prediction'] = (test['proba'] > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1219</td>\n",
       "      <td>44</td>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>236</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1326</td>\n",
       "      <td>280</td>\n",
       "      <td>1606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction     0    1   All\n",
       "depression                 \n",
       "0           1219   44  1263\n",
       "1            107  236   343\n",
       "All         1326  280  1606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 混淆矩阵\n",
    "\n",
    "pd.crosstab(test.depression, test.prediction, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurancy is 0.91\n"
     ]
    }
   ],
   "source": [
    "# - 计算准确率\n",
    "acc = sum(test['prediction'] == test['depression']) /np.float(len(test))\n",
    "print('The accurancy is %.2f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.1, precision: 0.77, recall:1.00 ,Specificity:0.54 , f1_score:0.87\n",
      "threshold: 0.2, precision: 0.88, recall:0.97 ,Specificity:0.68 , f1_score:0.92\n",
      "threshold: 0.30000000000000004, precision: 0.91, recall:0.95 ,Specificity:0.72 , f1_score:0.93\n",
      "threshold: 0.4, precision: 0.94, recall:0.95 ,Specificity:0.78 , f1_score:0.95\n",
      "threshold: 0.5, precision: 0.97, recall:0.92 ,Specificity:0.84 , f1_score:0.94\n",
      "threshold: 0.6, precision: 0.97, recall:0.91 ,Specificity:0.86 , f1_score:0.94\n",
      "threshold: 0.7000000000000001, precision: 0.97, recall:0.91 ,Specificity:0.87 , f1_score:0.94\n",
      "threshold: 0.8, precision: 1.00, recall:0.90 ,Specificity:0.99 , f1_score:0.94\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 0.9, 0.1):\n",
    "    prediction = (test['proba'] > i).astype('int')\n",
    "    confusion_matrix = pd.crosstab(prediction,test.depression,\n",
    "                                   margins = True)\n",
    "    precision = confusion_matrix.ix[0, 0] /confusion_matrix.ix['All', 0]\n",
    "    recall = confusion_matrix.ix[0, 0] / confusion_matrix.ix[0, 'All']\n",
    "    Specificity = confusion_matrix.ix[1, 1] /confusion_matrix.ix[1,'All']\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print('threshold: %s, precision: %.2f, recall:%.2f ,Specificity:%.2f , f1_score:%.2f'%(i, precision, recall, Specificity,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - 绘制ROC曲线\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "fpr_test, tpr_test, th_test = metrics.roc_curve(test.depression, test.proba)\n",
    "fpr_train, tpr_train, th_train = metrics.roc_curve(train.depression, train.proba)\n",
    "\n",
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(fpr_test, tpr_test, 'b--')\n",
    "plt.plot(fpr_train, tpr_train, 'r-')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9660\n"
     ]
    }
   ],
   "source": [
    "print('AUC = %.4f' %metrics.auc(fpr_test, tpr_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
